{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2cc376a5",
   "metadata": {},
   "source": [
    "### Info\n",
    "- Date: 2024-05-05\n",
    "- Author: Reshama S\n",
    "- Location: https://github.com/NoLaB-Lab/nlp-project1\n",
    "\n",
    "### Description\n",
    "- Evalute human vs ai transcripts\n",
    "\n",
    "### ROUGE score\n",
    "- A ROUGE score close to zero indicates poor similarity between candidate and references. \n",
    "- A ROUGE score close to one indicates strong similarity between candidate and references. \n",
    "- If candidate is identical to one of the reference documents, then score is 1.\n",
    "\n",
    "### Levenshtein score\n",
    "https://rapidfuzz.github.io/Levenshtein/levenshtein.html#distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c4e6991-bede-48bb-bccf-a99321aa4a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import pprint\n",
    "from Levenshtein import distance\n",
    "from Levenshtein import ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e0bd6c5-7ad2-4571-8286-fe8201ddf62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_human = \"../data/transcripts-clinician/\"\n",
    "dir_ai = \"../data/transcripts-whisper/\"\n",
    "\n",
    "dict_scores = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82c74dec-5f5c-4267-a83f-f7dbc99d7501",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ref: https://towardsdatascience.com/side-by-side-comparison-of-strings-in-python-b9491ac858\n",
    "\n",
    "import difflib\n",
    "import re\n",
    "\n",
    "def tokenize(s):\n",
    "    return re.split('\\s+', s)\n",
    "def untokenize(ts):\n",
    "    return ' '.join(ts)\n",
    "        \n",
    "def equalize(s1, s2):\n",
    "    l1 = tokenize(s1)\n",
    "    l2 = tokenize(s2)\n",
    "    res1 = []\n",
    "    res2 = []\n",
    "    prev = difflib.Match(0,0,0)\n",
    "    for match in difflib.SequenceMatcher(a=l1, b=l2).get_matching_blocks():\n",
    "        if (prev.a + prev.size != match.a):\n",
    "            for i in range(prev.a + prev.size, match.a):\n",
    "                res2 += ['_' * len(l1[i])]\n",
    "            res1 += l1[prev.a + prev.size:match.a]\n",
    "        if (prev.b + prev.size != match.b):\n",
    "            for i in range(prev.b + prev.size, match.b):\n",
    "                res1 += ['_' * len(l2[i])]\n",
    "            res2 += l2[prev.b + prev.size:match.b]\n",
    "        res1 += l1[match.a:match.a+match.size]\n",
    "        res2 += l2[match.b:match.b+match.size]\n",
    "        prev = match\n",
    "    return untokenize(res1), untokenize(res2)\n",
    "\n",
    "def insert_newlines(string, every=64, window=10):\n",
    "    result = []\n",
    "    from_string = string\n",
    "    while len(from_string) > 0:\n",
    "        cut_off = every\n",
    "        if len(from_string) > every:\n",
    "            while (from_string[cut_off-1] != ' ') and (cut_off > (every-window)):\n",
    "                cut_off -= 1\n",
    "        else:\n",
    "            cut_off = len(from_string)\n",
    "        part = from_string[:cut_off]\n",
    "        result += [part]\n",
    "        from_string = from_string[cut_off:]\n",
    "    return result\n",
    "\n",
    "def show_comparison(s1, s2, width=40, margin=10, sidebyside=True, compact=False):\n",
    "    s1, s2 = equalize(s1,s2)\n",
    "\n",
    "    if sidebyside:\n",
    "        s1 = insert_newlines(s1, width, margin)\n",
    "        s2 = insert_newlines(s2, width, margin)\n",
    "        if compact:\n",
    "            for i in range(0, len(s1)):\n",
    "                lft = re.sub(' +', ' ', s1[i].replace('_', '')).ljust(width)\n",
    "                rgt = re.sub(' +', ' ', s2[i].replace('_', '')).ljust(width) \n",
    "                print(lft + ' | ' + rgt + ' | ')        \n",
    "        else:\n",
    "            for i in range(0, len(s1)):\n",
    "                lft = s1[i].ljust(width)\n",
    "                rgt = s2[i].ljust(width)\n",
    "                print(lft + ' | ' + rgt + ' | ')\n",
    "    else:\n",
    "        print(s1)\n",
    "        print(s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "969abd45-a610-4a31-af7f-c55294e08a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readtext(filename):\n",
    "    #print(filename)\n",
    "    file = open(dir_human + filename + \".txt\", \"r\")\n",
    "    content_human = file.read()\n",
    "    file.close()\n",
    "    #print(content_human)\n",
    "    #print(\"-\" * 50)\n",
    "    \n",
    "    file_ai = open(dir_ai + filename + \".txt\", \"r\")\n",
    "    content_ai = file_ai.read()\n",
    "    file_ai.close()\n",
    "    #print(content_ai)\n",
    "    #print(\"-\" * 50)\n",
    "    return filename, content_human, content_ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b49e549-68c6-4bc0-bfbc-971c83ba171d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0180da8d-c06b-4ce1-a997-65d756790bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaltext(filename, content_human, content_ai):\n",
    "    #print(filename)\n",
    "    # load the metric (from Hugging Face)\n",
    "    score = evaluate.load('rouge')\n",
    "    #score = evaluate.load(\"accuracy\") # this gives error\n",
    "\n",
    "    results = score.compute(predictions=[content_ai],\n",
    "                         references=[content_human])\n",
    "    print(results)\n",
    "    dict_scores[filename] = results\n",
    "\n",
    "    disagreement = distance(content_human, content_ai)\n",
    "    print(f\"Levenshtein disagreement: {disagreement}\")\n",
    "    \n",
    "    ratiov = ratio(content_human, content_ai)\n",
    "    print(f\"Levensshtein ratio: {ratiov}\")\n",
    "\n",
    "    # Calculate normalized distance (between 0 and 1)\n",
    "    levenshtein_distance = distance(content_human, content_ai)\n",
    "    print(f\"Levenshtein distance: {levenshtein_distance}\")\n",
    "    sentence_length = max(len(content_human), len(content_ai))\n",
    "    normalized_distance = levenshtein_distance / sentence_length\n",
    "\n",
    "    print(f\"Normalized Levenshtein distance: {normalized_distance}\")\n",
    "\n",
    "    print('-' * 52)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f55bdf90-9a61-4bc7-9bbc-edb666ae24a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def comptext(filename, content_human, content_ai):\n",
    "    show_comparison(content_human, content_ai, width=50, sidebyside=True, compact=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c678355-a80a-4d58-81e5-15738d71c96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def runanalysis(patientnum):\n",
    "    print(patientnum)\n",
    "    filename, content_human, content_ai = readtext(patientnum)\n",
    "    evaltext(filename, content_human, content_ai)\n",
    "    comptext(filename, content_human, content_ai)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6fc0577c-1ab5-4ce9-9f36-83b6352649bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AJ_IMG_3334\n",
      "{'rouge1': 0.912751677852349, 'rouge2': 0.8224719101123596, 'rougeL': 0.9038031319910516, 'rougeLsum': 0.9038031319910516}\n",
      "Levenshtein disagreement: 156\n",
      "Levensshtein ratio: 0.9153952843273232\n",
      "Levenshtein distance: 156\n",
      "Normalized Levenshtein distance: 0.14156079854809436\n",
      "--------------------------------------------------\n",
      "I want you to tell me about any kind of trip that  | I want you to tell me about any kind of trip that  | \n",
      "you've taken. I know you've traveled a lot. So     | you've taken. I know you've traveled a lot. So     | \n",
      "tell me a little bit about, you know, a trip that  | tell me a little bit about, you know, a trip that  | \n",
      "you enjoyed. Well, we went to Italy. I enjoyed     | you enjoyed. Well, we went to Italy. I enjoyed     | \n",
      "Italy, enjoyed the food. I enjoyed the wine _____  | Italy, enjoyed the food. I enjoyed the ____ wine,  | \n",
      "and the women are fantastic. Um its ____ not the   | and the women are fantastic. __ ___ It's not the   | \n",
      "safest place in the world ______ but you kind of   | safest place in the _____ world, but you kind of   | \n",
      "live with, you ____ ___ ___ kind ___ you live      | live _____ ___ with it. You kind of, you live      | \n",
      "with. ____ ___ You know you're supposed to lock    | _____ with it. You know you're supposed to lock    | \n",
      "your house and close your windows and all that     | your house and close your windows and all that     | \n",
      "staff. Okay. And but ______ ___ everything was     | ______ _____ ___ ___ stuff. But everything was     | \n",
      "fine I mean. _____ Did you live there or           | ____ _ _____ fine. Did you live there or           | \n",
      "vacation? No, I lived there. ____ Stayed there     | vacation? No, I lived there. Wow. Stayed there     | \n",
      "for about three years. What part? Naples. _______  | for about three years. What part? Naples. Naples.  | \n",
      "Lovely place. But ____ like I said its not, its    | Lovely place. ___ But, like I ____ ___ ____ ___    | \n",
      "_____ ____ not the safest place to live. ____ Did  | said, it's not the safest place to live. But. Did  | \n",
      "you speak Italian? I spoke a little bit, yeah, a   | you speak Italian? I spoke a little bit, _____ _   | \n",
      "_____ ____ _ little bit. Did ___ ___ you work      | yeah. Wow. A little bit. ___ And did you work      | \n",
      "there or now? Yes ___ ____ I did. I worked at the  | there or ____ ___ no? Yes, I did. I worked at ___  | \n",
      "_ gym. Cool. And what's ______ your wife do?       | a gym. Cool. And ______ what'd your wife do?       | \n",
      "Because you mentioned it was her job. ____ ______  | Because you mentioned it was her ____ job, right?  | \n",
      "___ ____ _____ She yeah she was uh, uh ____ she    | Oh, she, yeah. She ____ ___ ___ ___ __ was, she    | \n",
      "ran a tourist place, you know _____ where people   | ran a tourist place, you ____ know, where people   | \n",
      "want to take a trip. She had a lot of people to    | want to take a trip. She had a lot of people __    | \n",
      "work ____ ______ for her ____ but people want to   | ____ that worked for ___ her, but people want to   | \n",
      "take a trip or whatever and _________ ____ she     | take a trip or ________ ___ whatever, then she     | \n",
      "set it up. ___ _____ _____ _____                   | set it up. How neat. Yeah. Good.                   | \n"
     ]
    }
   ],
   "source": [
    "runanalysis(\"AJ_IMG_3334\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9f98e99a-fead-4e42-a645-eff6f02bec78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF_IMG_3241\n",
      "{'rouge1': 0.9210526315789473, 'rouge2': 0.847682119205298, 'rougeL': 0.9210526315789473, 'rougeLsum': 0.9144736842105264}\n",
      "Levenshtein disagreement: 81\n",
      "Levensshtein ratio: 0.9309878213802436\n",
      "Levenshtein distance: 81\n",
      "Normalized Levenshtein distance: 0.10857908847184987\n",
      "--------------------------------------------------\n",
      " I'm also going to ask you to tell me a story      |  I'm also going to ask you to tell me a story      | \n",
      "about a bad, hopefully not too traumatic, but      | about a bad, hopefully not too traumatic, but      | \n",
      "like a bad childhood memory _______ if you have    | like a bad childhood ______ memory, if you have    | \n",
      "any ____ or adolescent ____________ something      | ___ any, or __________ adolescence, something      | \n",
      "that happened that wasn't too great ______ ___ I   | that happened that wasn't ___ _____ great. Oh, I   | \n",
      "remember when I was very young. well ______ _____  | remember when I was very ______ ____ young, well,  | \n",
      "not very _____ maybe about 10 years old ____ my    | not ____ very, maybe about 10 years ___ old, my    | \n",
      "father worked in a summer camp and a few of us     | father worked in a summer camp and a few of us     | \n",
      "kids were horsing around and we were climbing out  | kids were horsing around and we were climbing out  | \n",
      "of a window and I sort of fell and hit my wrist    | of a window and I sort of fell and hit my wrist    | \n",
      "and broke my wrist which ______ _____ made my the  | and broke my _____ _____ wrist. Which made __ the  | \n",
      "rest of the summer this _______ _ was in the       | rest of the ______ ____ summer, I was in the       | \n",
      "beginning of the summer _______ not too much fun   | beginning of the ______ summer, not too much fun   | \n",
      "cause _______ I had a cast on my wrist for like    | _____ because I had a cast on my wrist for like    | \n",
      "three-quarters _____ ________ of the summr oh      | ______________ three quarters of the _____ __      | \n",
      "_______ __ my goodness did _________ ___ your      | summer. Oh my ________ ___ goodness. Did your      | \n",
      "dad, did you get in trouble for it or yeah ___     | dad, did you get in trouble for __ __ ____ it?     | \n",
      "_____ ______ minor minor trouble yeah lets see     | Yeah, minor, minor _____ _______ ____ ____ ___     | \n",
      "thats okay , thats fine  ________ ______ _____     | _____ ____ _ _____ ____  trouble. Yikes, let's     | \n",
      "____ ____ _____ ______ _____ ________              | see. It's okay, that's fine. Alright.              | \n"
     ]
    }
   ],
   "source": [
    "# highest score\n",
    "runanalysis(\"RF_IMG_3241\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "171b594f-51a2-4c0d-8da2-8ba91185183a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SS_IMG_2863\n",
      "{'rouge1': 0.7400881057268723, 'rouge2': 0.5866666666666667, 'rougeL': 0.7312775330396477, 'rougeLsum': 0.7312775330396477}\n",
      "Levenshtein disagreement: 208\n",
      "Levensshtein ratio: 0.7803521779425394\n",
      "Levenshtein distance: 208\n",
      "Normalized Levenshtein distance: 0.3382113821138211\n",
      "--------------------------------------------------\n",
      "Now One  ___ ___ more thing I'm going ____ ___     | ___ ___  And one more thing ___ _____ I'll ask     | \n",
      "___ to have you tell me _____ is a childhood       | you to ____ ___ tell me about is a childhood       | \n",
      "memory that wasn't good, that was sad, ____ _ ___  | memory that wasn't good, ____ ___ ____ like a sad  | \n",
      "___ ___ _____ hopefully not too dramatic Yeah      | or, you know, hopefully not too ________ ____      | \n",
      "__________ ___ ____ _ ___ ______ ____ __________   | traumatic, but like a bad memory from childhood.   | \n",
      "_____ I have ___ _ ______ fairly good, you know,   | Yeah. I ____ had a pretty fairly good, you know,   | \n",
      "no issues that I can really remeber _________ but  | no issues that I can really _______ remember, but  | \n",
      "I do remember falling out of a tree. Thank god     | I do remember falling out of a tree. Thank ___     | \n",
      "___ it wasnt ______ too high Reaching _____ ___    | God it _____ wasn't too ____ ________ high. You    | \n",
      "_____ ________ for the next one and the next one   | know, reaching for the next one and the next ___   | \n",
      "andall ____ ___ ____ ___ of a sudden ______        | ______ one, and then all of a sudden you're        | \n",
      "________ ___ you don't get it ___ and \"pew bam\"    | reaching and you don't get __ it, and ____ ____    | \n",
      "____ ____ It wasn't horrible _________ but it      | you, bam. It wasn't ________ horrible, but it      | \n",
      "hurt _____ ______ __ _____ _____ Did you break     | ____ hurt. Scary. It hurt, yeah. Did you break     | \n",
      "any thing? _________ No, I _______ ___ ____ never  | ___ ______ anything? No, I didn't. No, I've never  | \n",
      "had a broken bone _____ _____ Thank god  ____      | had a broken ____ bone. Good. Thank ___  God.      | \n",
      "_____                                              | Okay.                                              | \n"
     ]
    }
   ],
   "source": [
    "# lowest score\n",
    "runanalysis(\"SS_IMG_2863\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7d5e44-3a92-4289-a661-6e57659b640a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "920a25db-7f5f-4c2a-8c66-5349e5b58dc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n",
      "{'rouge1': 1.0, 'rouge2': 1.0, 'rougeL': 1.0, 'rougeLsum': 1.0}\n",
      "Levenshtein disagreement: 0\n",
      "Levensshtein ratio: 1.0\n",
      "Levenshtein distance: 0\n",
      "Normalized Levenshtein distance: 0.0\n",
      "--------------------------------------------------\n",
      "I want you to tell me about any kind of trip that  | I want you to tell me about any kind of trip that  | \n",
      "you've taken. I know you've traveled a lot. So     | you've taken. I know you've traveled a lot. So     | \n",
      "tell me a little bit about, you know, a trip that  | tell me a little bit about, you know, a trip that  | \n",
      "you enjoyed. Well, we went to Italy. I enjoyed     | you enjoyed. Well, we went to Italy. I enjoyed     | \n",
      "Italy, enjoyed the food. I enjoyed the wine and    | Italy, enjoyed the food. I enjoyed the wine and    | \n",
      "the women are fantastic. Um its not the safest     | the women are fantastic. Um its not the safest     | \n",
      "place in the world but you kind of live with, you  | place in the world but you kind of live with, you  | \n",
      "kind you live with. You know you're supposed to    | kind you live with. You know you're supposed to    | \n",
      "lock your house and close your windows and all     | lock your house and close your windows and all     | \n",
      "that staff. Okay. And but everything was fine I    | that staff. Okay. And but everything was fine I    | \n",
      "mean. Did you live there or vacation? No, I lived  | mean. Did you live there or vacation? No, I lived  | \n",
      "there. Stayed there for about three years. What    | there. Stayed there for about three years. What    | \n",
      "part? Naples. Lovely place. But like I said its    | part? Naples. Lovely place. But like I said its    | \n",
      "not, its not the safest place to live.             | not, its not the safest place to live.             | \n"
     ]
    }
   ],
   "source": [
    "# Baseline: both texts are the same\n",
    "runanalysis(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6df002e3-2fb2-4f36-b1f8-df2fe7a1aea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test1\n",
      "{'rouge1': 1.0, 'rouge2': 1.0, 'rougeL': 1.0, 'rougeLsum': 1.0}\n",
      "Levenshtein disagreement: 6\n",
      "Levensshtein ratio: 0.9955089820359282\n",
      "Levenshtein distance: 6\n",
      "Normalized Levenshtein distance: 0.00894187779433681\n",
      "--------------------------------------------------\n",
      "I want you to tell me about any kind of trip that  | I want you to tell me about any kind of trip that  | \n",
      "you've taken. I know you've traveled a lot. So     | you've taken. I know you've traveled a lot. So     | \n",
      "tell me a little bit about, _____ you know, ____   | tell me a little bit ______ about you _____ know   | \n",
      "a trip that you enjoyed. Well, we went to Italy.   | a trip that you enjoyed. Well, we went to Italy.   | \n",
      "I enjoyed Italy, _____ enjoyed the food. I         | I enjoyed ______ Italy enjoyed the food. I         | \n",
      "enjoyed the wine and the women are fantastic. Um   | enjoyed the wine and the women are fantastic. Um   | \n",
      "its not the safest place in the world but you      | its not the safest place in the world but you      | \n",
      "kind of live with, ____ you kind you live with.    | kind of live _____ with you kind you live with.    | \n",
      "You know you're supposed to lock your house and    | You know you're supposed to lock your house and    | \n",
      "close your windows and all that staff. Okay. And   | close your windows and all that staff. Okay. And   | \n",
      "but everything was fine I mean. Did you live       | but everything was fine I mean. Did you live       | \n",
      "there or vacation? No, __ I lived there. Stayed    | there or vacation? ___ No I lived there. Stayed    | \n",
      "there for about three years. What part? Naples.    | there for about three years. What part? Naples.    | \n",
      "Lovely place. But like I said its not, ___ its     | Lovely place. But like I said its ____ not its     | \n",
      "not the safest place to live.                      | not the safest place to live.                      | \n"
     ]
    }
   ],
   "source": [
    "# remove commas\n",
    "runanalysis(\"test1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b155e3b8-95bf-4721-88eb-67ed2d04b475",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test2\n",
      "{'rouge1': 1.0, 'rouge2': 1.0, 'rougeL': 1.0, 'rougeLsum': 1.0}\n",
      "Levenshtein disagreement: 22\n",
      "Levensshtein ratio: 0.9833333333333333\n",
      "Levenshtein distance: 22\n",
      "Normalized Levenshtein distance: 0.03278688524590164\n",
      "--------------------------------------------------\n",
      "I want you to tell me about any kind of trip that  | I want you to tell me about any kind of trip that  | \n",
      "you've taken. _____ I know you've traveled a lot.  | you've ______ taken I know you've traveled a lot.  | \n",
      "So tell me a little bit about, _____ you know,     | So tell me a little bit ______ about you _____     | \n",
      "____ a trip that you enjoyed. _______ Well, we     | know a trip that you ________ enjoyed Well, we     | \n",
      "went to Italy. _____ I enjoyed Italy, _____        | went to ______ Italy I enjoyed ______ Italy        | \n",
      "enjoyed the food. ____ I enjoyed the wine and the  | enjoyed the _____ food I enjoyed the wine and the  | \n",
      "women are fantastic. _________ Um its not the      | women are __________ fantastic Um its not the      | \n",
      "safest place in the world but you kind of live     | safest place in the world but you kind of live     | \n",
      "with, ____ you kind you live with. ____ You know   | _____ with you kind you live _____ with You know   | \n",
      "you're supposed to lock your house and close your  | you're supposed to lock your house and close your  | \n",
      "windows and all that staff. Okay. _____ ____ And   | windows and all that ______ _____ staff Okay And   | \n",
      "but everything was fine I mean. ____ Did you live  | but everything was fine I _____ mean Did you live  | \n",
      "there or vacation? No, ________ __ I lived there.  | there or _________ ___ vacation No I lived ______  | \n",
      "_____ Stayed there for about three years. _____    | there Stayed there for about three ______ years    | \n",
      "What part? Naples. ____ ______ Lovely place.       | What _____ _______ part Naples Lovely ______       | \n",
      "_____ But like I said its not, ___ its not the     | place But like I said its ____ not its not the     | \n",
      "safest place to live. ____                         | safest place to _____ live                         | \n"
     ]
    }
   ],
   "source": [
    "# remove all periods\n",
    "runanalysis(\"test2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0071a841-ee3d-45e6-8e36-8bbfa91c7b36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test3\n",
      "{'rouge1': 0.9444444444444444, 'rouge2': 0.9428571428571428, 'rougeL': 0.9444444444444444, 'rougeLsum': 0.9444444444444444}\n",
      "Levenshtein disagreement: 10\n",
      "Levensshtein ratio: 0.9691211401425178\n",
      "Levenshtein distance: 10\n",
      "Normalized Levenshtein distance: 0.04672897196261682\n",
      "--------------------------------------------------\n",
      "two-thirds ___ vs 2/3. Capital Letters vs capital  | __________ 2/3 vs 2/3. Capital Letters vs capital  | \n",
      "letters. Misspelled words vs mispelled words.      | letters. Misspelled words vs mispelled words.      | \n",
      "Three-quarters vs three quarters. Ten vs 10.       | Three-quarters vs three quarters. Ten vs 10.       | \n",
      "Eleven vs 11. Italy vs italy Ummm vs Punction      | Eleven vs 11. Italy vs italy Ummm vs Punction      | \n",
      "included. vs no punctuation                        | included. vs no punctuation                        | \n"
     ]
    }
   ],
   "source": [
    "# texting types of words\n",
    "runanalysis(\"test3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31ced2f-31b2-4f3c-841b-c0e7ae3d56d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7422de57-dfeb-4e8a-986f-e9dadd6d13f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd53018e-1594-47e5-a646-8c9fcdf73f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaltext(\"AJ_IMG_3334\")\n",
    "evaltext(\"AJ_IMG_3335\")\n",
    "evaltext(\"AP_IMG_3383\")\n",
    "evaltext(\"AP_IMG_3384\")\n",
    "evaltext(\"BM_IMG_3480\")\n",
    "\n",
    "evaltext(\"BM_IMG_3481\")\n",
    "evaltext(\"MW_IMG_3200\")\n",
    "evaltext(\"MW_IMG_3201\")\n",
    "evaltext(\"PG_IMG_3189\")\n",
    "evaltext(\"PG_IMG_3190\")\n",
    "\n",
    "evaltext(\"RF_IMG_3240\")\n",
    "evaltext(\"RF_IMG_3241\")\n",
    "evaltext(\"SS_IMG_2862\")\n",
    "evaltext(\"SS_IMG_2863\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6637ad70-fa59-4b01-b43b-6cf7cbdc2403",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5383e1-16b5-437c-bb67-47dd8e2c2cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dict_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0c5711-83ca-4ac7-a532-078dd32dc2d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf979da-a5d3-4d51-bc68-3d4fd238a374",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84e8cd4-16e8-4dce-baca-94facd60031d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10020c94-724b-4d32-a2a1-430f8056a9f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3bdb8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7480e346",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7425edfa-85d1-48c0-a575-ed81389a86e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31495466-acbe-4ee7-b6ba-18a781f61247",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
