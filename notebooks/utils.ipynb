{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2cc376a5",
   "metadata": {},
   "source": [
    "### Date updated\n",
    "2024-01-04\n",
    "\n",
    "### Authors\n",
    "Reshama S\n",
    "\n",
    "### Instructions for Whisper\n",
    "- https://github.com/openai/whisper\n",
    "- Setting up virtual environment: https://packaging.python.org/en/latest/guides/installing-using-pip-and-virtual-environments/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41e1c110",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import shlex\n",
    "import subprocess\n",
    "from collections import Counter\n",
    "from functools import reduce\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from nltk.corpus import words\n",
    "\n",
    "#import docx\n",
    "import spacy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89de2a69-a179-40af-9ea4-1788442afca4",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[E053] Could not read config file from /Users/reshamas/opt/anaconda3/envs/nlp-project1/lib/python3.11/site-packages/en_core_web_sm/en_core_web_sm-2.2.0/config.cfg",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01men_core_web_sm\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m nlp \u001b[38;5;241m=\u001b[39m \u001b[43men_core_web_sm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/nlp-project1/lib/python3.11/site-packages/en_core_web_sm/__init__.py:12\u001b[0m, in \u001b[0;36mload\u001b[0;34m(**overrides)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moverrides):\n\u001b[0;32m---> 12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mload_model_from_init_py\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;18;43m__file__\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43moverrides\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/nlp-project1/lib/python3.11/site-packages/spacy/util.py:682\u001b[0m, in \u001b[0;36mload_model_from_init_py\u001b[0;34m(init_file, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m    680\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m model_path\u001b[38;5;241m.\u001b[39mexists():\n\u001b[1;32m    681\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE052\u001b[38;5;241m.\u001b[39mformat(path\u001b[38;5;241m=\u001b[39mdata_path))\n\u001b[0;32m--> 682\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mload_model_from_path\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    683\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    684\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvocab\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    685\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmeta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmeta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    686\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdisable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    687\u001b[0m \u001b[43m    \u001b[49m\u001b[43menable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    688\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexclude\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    689\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    690\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/nlp-project1/lib/python3.11/site-packages/spacy/util.py:538\u001b[0m, in \u001b[0;36mload_model_from_path\u001b[0;34m(model_path, meta, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m    536\u001b[0m config_path \u001b[38;5;241m=\u001b[39m model_path \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig.cfg\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    537\u001b[0m overrides \u001b[38;5;241m=\u001b[39m dict_to_dot(config, for_overrides\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 538\u001b[0m config \u001b[38;5;241m=\u001b[39m \u001b[43mload_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverrides\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverrides\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    539\u001b[0m nlp \u001b[38;5;241m=\u001b[39m load_model_from_config(\n\u001b[1;32m    540\u001b[0m     config,\n\u001b[1;32m    541\u001b[0m     vocab\u001b[38;5;241m=\u001b[39mvocab,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    545\u001b[0m     meta\u001b[38;5;241m=\u001b[39mmeta,\n\u001b[1;32m    546\u001b[0m )\n\u001b[1;32m    547\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m nlp\u001b[38;5;241m.\u001b[39mfrom_disk(model_path, exclude\u001b[38;5;241m=\u001b[39mexclude, overrides\u001b[38;5;241m=\u001b[39moverrides)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/nlp-project1/lib/python3.11/site-packages/spacy/util.py:714\u001b[0m, in \u001b[0;36mload_config\u001b[0;34m(path, overrides, interpolate)\u001b[0m\n\u001b[1;32m    712\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    713\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m config_path \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m config_path\u001b[38;5;241m.\u001b[39mis_file():\n\u001b[0;32m--> 714\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE053\u001b[38;5;241m.\u001b[39mformat(path\u001b[38;5;241m=\u001b[39mconfig_path, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig file\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m    715\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m config\u001b[38;5;241m.\u001b[39mfrom_disk(\n\u001b[1;32m    716\u001b[0m         config_path, overrides\u001b[38;5;241m=\u001b[39moverrides, interpolate\u001b[38;5;241m=\u001b[39minterpolate\n\u001b[1;32m    717\u001b[0m     )\n",
      "\u001b[0;31mOSError\u001b[0m: [E053] Could not read config file from /Users/reshamas/opt/anaconda3/envs/nlp-project1/lib/python3.11/site-packages/en_core_web_sm/en_core_web_sm-2.2.0/config.cfg"
     ]
    }
   ],
   "source": [
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75699058",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'en_core_web_lg'. It doesn't seem to be a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m nlp \u001b[38;5;241m=\u001b[39m \u001b[43mspacy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43men_core_web_lg\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m spacy\u001b[38;5;241m.\u001b[39mtokens\u001b[38;5;241m.\u001b[39mToken\u001b[38;5;241m.\u001b[39mset_extension(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeature\u001b[39m\u001b[38;5;124m'\u001b[39m, default\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m      5\u001b[0m WORD_SET \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(words\u001b[38;5;241m.\u001b[39mwords())\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/nlp-project1/lib/python3.11/site-packages/spacy/__init__.py:51\u001b[0m, in \u001b[0;36mload\u001b[0;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\n\u001b[1;32m     28\u001b[0m     name: Union[\u001b[38;5;28mstr\u001b[39m, Path],\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     34\u001b[0m     config: Union[Dict[\u001b[38;5;28mstr\u001b[39m, Any], Config] \u001b[38;5;241m=\u001b[39m util\u001b[38;5;241m.\u001b[39mSimpleFrozenDict(),\n\u001b[1;32m     35\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Language:\n\u001b[1;32m     36\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load a spaCy model from an installed package or a local path.\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \n\u001b[1;32m     38\u001b[0m \u001b[38;5;124;03m    name (str): Package name or model path.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;124;03m    RETURNS (Language): The loaded nlp object.\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvocab\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdisable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m        \u001b[49m\u001b[43menable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexclude\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/nlp-project1/lib/python3.11/site-packages/spacy/util.py:472\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m    470\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m OLD_MODEL_SHORTCUTS:\n\u001b[1;32m    471\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE941\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname, full\u001b[38;5;241m=\u001b[39mOLD_MODEL_SHORTCUTS[name]))  \u001b[38;5;66;03m# type: ignore[index]\u001b[39;00m\n\u001b[0;32m--> 472\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE050\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname))\n",
      "\u001b[0;31mOSError\u001b[0m: [E050] Can't find model 'en_core_web_lg'. It doesn't seem to be a Python package or a valid path to a data directory."
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "spacy.tokens.Token.set_extension('feature', default=None)\n",
    "\n",
    "WORD_SET = set(words.words())\n",
    "\n",
    "def read_data(PATH, print_exception=False):\n",
    "    data = pd.DataFrame(columns=['pidn', 'variant', 'text']).set_index('pidn')\n",
    "\n",
    "    for variant in os.listdir(PATH):\n",
    "        for fname in os.listdir(os.path.join(PATH, variant)):\n",
    "            try:\n",
    "                pidn = int(fname[:-4])\n",
    "                with open(os.path.join(os.path.join(PATH, variant), fname), 'r') as f:\n",
    "                    text = f.read()\n",
    "                data.loc[(pidn, 0), :] = variant, text\n",
    "            except Exception as e:\n",
    "                if print_exception:\n",
    "                    print(pidn, e)\n",
    "\n",
    "    data = data.sort_index()\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ec9a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "!which python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1e5b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda env list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c53f123",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_docx(PATH, affix='', print_exception=False):\n",
    "    data = pd.DataFrame(columns=['pidn', 'visit', 'variant', 'raw text']).set_index(['pidn', 'visit'])\n",
    "\n",
    "    for variant in os.listdir(PATH):\n",
    "        if variant == '.DS_Store':\n",
    "            continue\n",
    "        for fname in os.listdir(os.path.join(PATH, variant + affix)):\n",
    "            try:\n",
    "                pidn = int(fname[:5])\n",
    "                if variant == 'long_nfvPPA':\n",
    "                    visit = 1\n",
    "                else:\n",
    "                    visit = 0\n",
    "                doc = docx.Document(os.path.join(os.path.join(PATH, variant + affix), fname))\n",
    "                text = '\\n'.join([x.text for x in doc.paragraphs if x.text != ''])\n",
    "                data.loc[(pidn ,visit), :] = variant, text\n",
    "            except Exception as e:\n",
    "                if print_exception:\n",
    "                    print(pidn, e)\n",
    "\n",
    "    data = data.sort_index()\n",
    "\n",
    "    return data\n",
    "\n",
    "def get_ft_label(text):\n",
    "\n",
    "    return subprocess.check_output(\"\"\"~/fastText/fasttext predict-prob ~/fastText/amazon_review_polarity.bin - <<< {}\"\"\".format(shlex.quote(text)), shell=True).decode('utf8')[:-1]\n",
    "\n",
    "# def aggregate(data):\n",
    "\n",
    "#     data.loc[:, 'sentiment'] = pd.Series(get_ft_label('\\n'.join(data.loc[:, 'text'].apply(lambda x: x.replace('\\n', '')))).split('\\n'), index=data.index).str[9].astype(int) - 1\n",
    "\n",
    "#     parts_of_speech = reduce(lambda x, y: x.append(y, sort=True), data.loc[:, 'doc'].apply(lambda x: pd.DataFrame({k: [v] for k, v in dict(Counter([token.pos_ for token in x])).items()})))\n",
    "#     parts_of_speech.index = data.index\n",
    "#     parts_of_speech = parts_of_speech.fillna(0)\n",
    "#     data = data.join(parts_of_speech)\n",
    "\n",
    "#     features = reduce(lambda x, y: x.append(y, sort=True), data.loc[:, 'doc'].apply(lambda x: pd.DataFrame({k: [v] for k, v in dict(Counter([token._.feature for token in x if token._.feature])).items()})))\n",
    "#     features.index = data.index\n",
    "#     features = features.fillna(0)\n",
    "#     data = data.join(features)\n",
    "\n",
    "#     return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9802f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher1 = spacy.matcher.Matcher(nlp.vocab, validate=True)\n",
    "matcher1.add('there be', [[{'LOWER': 'there'}, {'POS': 'AUX'}]])\n",
    "matcher1.add('i see', [[{'LOWER': 'i'}, {'POS': 'VERB', 'LEMMA': 'see'}]])\n",
    "matcher1.add('not argument', [[{'LOWER': 'i'}, {'POS': 'VERB', 'LEMMA': 'see'}, {'DEP': {'IN': ['det', 'amod', 'advmod']}, 'OP': '*'}, {'DEP': 'dobj'}]])\n",
    "matcher1.add('appear to be', [[{'LEMMA': 'appear'}, {'LEMMA': 'to'}, {'LEMMA': 'be'}]])\n",
    "\n",
    "matcher2 = spacy.matcher.Matcher(nlp.vocab, validate=True)\n",
    "matcher2.add('aux', [[{'POS': 'AUX', 'DEP': {'IN': ['aux', 'auxpass']}}]])\n",
    "matcher2.add('copula', [[{'POS': 'AUX', 'DEP': {'NOT_IN': ['aux', 'auxpass']}}, {'POS': {'IN': ['ADV', 'PART']}, 'OP': '*'}, {'POS': {'NOT_IN': ['ADV', 'PART']}, 'DEP': {'NOT_IN': ['cc']}}]])\n",
    "matcher2.add('verb', [[{'POS': 'VERB'}]])\n",
    "matcher2.add('argument', [[{'DEP': 'dobj'}], [{'LOWER': 'i'}, {'POS': 'VERB', 'LEMMA': 'see'}, {'DEP': {'IN': ['det', 'amod', 'advmod']}, 'OP': '*'}, {'DEP': 'nsubj'}]])\n",
    "matcher2.add('predicate', [[{'POS': 'AUX'}, {'DEP': {'IN': ['det', 'amod', 'advmod']}, 'OP': '*'}, {'DEP': 'attr'}]])\n",
    "matcher2.add('adjunct', [[{'DEP': 'pobj'}]])\n",
    "\n",
    "# def add_feature(doc):\n",
    "\n",
    "#     for x in matcher1(doc):\n",
    "#         if nlp.vocab.strings[x[0]] in ('there be', 'i see', 'not argument'):\n",
    "#             doc[x[1]:x[2]][-1]._.feature = nlp.vocab.strings[x[0]]\n",
    "#         elif not doc[x[1]:x[2]][0]._.feature:\n",
    "#             doc[x[1]:x[2]][0]._.feature = nlp.vocab.strings[x[0]]\n",
    "\n",
    "#     for x in matcher2(doc):\n",
    "#         if nlp.vocab.strings[x[0]] in ('predicate', 'argument') and not doc[x[1]:x[2]][-1]._.feature:\n",
    "#             doc[x[1]:x[2]][-1]._.feature = nlp.vocab.strings[x[0]]\n",
    "#         elif not doc[x[1]:x[2]][0]._.feature:\n",
    "#             doc[x[1]:x[2]][0]._.feature = nlp.vocab.strings[x[0]]\n",
    "\n",
    "def add_feature(doc):\n",
    "\n",
    "    def visit(word):\n",
    "\n",
    "        if word.pos_ == 'AUX':\n",
    "            if word.dep_ == 'aux':\n",
    "                word._.feature = 'aux' # Syntactic auxiliary verb\n",
    "            elif word.n_lefts > 0 and next(word.lefts).lemma_ == 'there':\n",
    "                word._.feature = 'there be'\n",
    "            elif word.n_rights > 0 and next(word.rights).dep_ != 'cc':\n",
    "                word._.feature = 'copula'\n",
    "            else:\n",
    "                word._.feature = 'other aux'\n",
    "        elif word.pos_ == 'VERB':\n",
    "            if word.lemma_ == 'see' and word.n_lefts > 0 and next(word.lefts).text.upper() == 'I':\n",
    "                word._.feature = 'i see'\n",
    "            elif word.lemma_ != 'see' and word.n_lefts > 0 and next(word.lefts).text.upper() == 'I':\n",
    "                word._.feature = 'i ...'\n",
    "            elif word.lemma_ == 'appear' and word.n_rights > 0 and next(word.rights).pos_ == 'AUX':\n",
    "                word._.feature = 'appear to be'\n",
    "            elif word.lemma_ == 'look' and word.n_rights > 0 and next(word.rights).pos_ == 'SCONJ':\n",
    "                word._.feature = 'look like'\n",
    "            elif word.dep_ == 'ccomp':\n",
    "                word._.feature = 'verb'\n",
    "            else:\n",
    "                word._.feature = 'verb'\n",
    "        elif word.pos_ == 'NOUN' or word.pos_ == 'NUM':\n",
    "            try:\n",
    "                anc = next(word.ancestors)\n",
    "                if anc.pos_ == 'VERB' and word.dep_ == 'dobj':\n",
    "                    if anc._.feature and anc._.feature == 'i see':\n",
    "                        word._.feature = 'predicate'\n",
    "                    else:\n",
    "                        word._.feature = 'argument'\n",
    "                elif anc.pos_ == 'ADP' and word.dep_ == 'pobj':\n",
    "                    if anc.text.lower() != 'of':\n",
    "                        try:\n",
    "                            anc_anc = next(anc.ancestors)\n",
    "                            if anc_anc.pos_ == 'AUX' and len([x for x in anc_anc.rights if x.pos_ == 'ADP']) == 1:\n",
    "                                word._.feature = 'argument'\n",
    "                            else:\n",
    "                                word._.feature = 'adjunct'\n",
    "                        except:\n",
    "                            word._.feature = 'adjunct'\n",
    "                    else:\n",
    "                        try:\n",
    "                            anc_anc = next(anc.ancestors)\n",
    "                            if anc_anc._.feature:\n",
    "                                word._.feature = anc_anc._.feature\n",
    "                                anc_anc._.feature = '... of'\n",
    "                        except:\n",
    "                            pass\n",
    "                elif anc.pos_ == 'AUX' and word.dep_ == 'attr' or word.dep_ == 'dobj':\n",
    "                    word._.feature = 'predicate'\n",
    "                elif anc.pos_ == 'SCONJ' and word.dep_ == 'pobj':\n",
    "                    try:\n",
    "                        anc_anc = next(anc.ancestors)\n",
    "                        if anc_anc.pos_ == 'AUX' or anc_anc.lemma_ == 'look':\n",
    "                            word._.feature = 'predicate'\n",
    "                        else:\n",
    "                            word._.feature = 'adjunct'\n",
    "                    except:\n",
    "                        pass\n",
    "                elif anc._.feature and 'conj' not in anc._.feature and word.dep_ == 'conj':\n",
    "                    if anc.pos_ != 'VERB':\n",
    "                        word._.feature = anc._.feature + ' conj'\n",
    "                    else:\n",
    "                        try:\n",
    "                            anc_anc = next(anc.ancestors)\n",
    "                            if anc_anc._.feature and anc_anc.pos_ != 'VERB':\n",
    "                                word._.feature = anc_anc._.feature + ' conj'\n",
    "                        except:\n",
    "                            pass\n",
    "            except:\n",
    "                pass\n",
    "        elif word.pos_ == 'ADV' and word.dep_ == 'advmod':\n",
    "            try:\n",
    "                anc = next(word.ancestors)\n",
    "                if anc.pos_ not in ('ADV', 'AUX'):\n",
    "                    word._.feature = 'adv adjunct'\n",
    "                elif anc.pos_ == 'AUX':\n",
    "                    n_nouns = 0\n",
    "                    n_advs = 0\n",
    "                    for right in anc.rights:\n",
    "                        if right.pos_ == 'NOUN':\n",
    "                            n_nouns += 1\n",
    "                        elif right.pos_ == 'ADV':\n",
    "                            n_advs += 1\n",
    "                    if n_nouns > 0:\n",
    "                        word._.feature = 'adv adjunct'\n",
    "                    elif n_advs == 1:\n",
    "                        word._.feature = 'predicate'\n",
    "            except:\n",
    "                pass\n",
    "        elif word.pos_ == 'ADJ' and word.dep_ == 'acomp':\n",
    "            word._.feature = 'predicate'\n",
    "\n",
    "        if word._.feature == 'adjunct':\n",
    "            try:\n",
    "                anc = next(word.ancestors)\n",
    "                if anc.lemma_ == 'into':\n",
    "                    try:\n",
    "                        anc_anc = next(anc.ancestors)\n",
    "                        if anc_anc.lemma_ == 'pour':\n",
    "                            word._.feature = 'argument'\n",
    "                    except:\n",
    "                        pass\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        for child in word.children:\n",
    "            visit(child)\n",
    "\n",
    "\n",
    "    for sent in doc.sents:\n",
    "        if len(sent) > 0:\n",
    "            visit(sent.root)\n",
    "\n",
    "def get_text_with_features(doc):\n",
    "\n",
    "    def join_punctuation(seq, characters='.,;?!'):\n",
    "        characters = set(characters)\n",
    "        seq = iter(seq)\n",
    "        current = next(seq)\n",
    "\n",
    "        for nxt in seq:\n",
    "            if nxt in characters:\n",
    "                current += nxt\n",
    "            else:\n",
    "                yield current\n",
    "                current = nxt\n",
    "\n",
    "        yield current\n",
    "\n",
    "    result = ' '.join(join_punctuation([token.text + ('<{}>'.format(token._.feature) if token._.feature else '') for token in doc]))\n",
    "    result = re.sub(r'\\n\\ +', '\\n', result)\n",
    "    return result\n",
    "\n",
    "def get_text_with_pos(doc, pos='pos'):\n",
    "\n",
    "    def join_punctuation(seq, characters='.,;?!'):\n",
    "        characters = set(characters)\n",
    "        seq = iter(seq)\n",
    "        current = next(seq)\n",
    "\n",
    "        for nxt in seq:\n",
    "            if nxt in characters:\n",
    "                current += nxt\n",
    "            else:\n",
    "                yield current\n",
    "                current = nxt\n",
    "\n",
    "        yield current\n",
    "\n",
    "    result = ' '.join(join_punctuation([token.text + ('<{}>'.format(token.pos_ if pos == 'pos' else token.tag_ if pos == 'tag' else token.pos_ + ' ' + token.tag_) if not token.pos_ in ('PUNCT', 'SPACE')  else '') for token in doc]))\n",
    "    result = re.sub(r'\\n\\ +', '\\n', result)\n",
    "    return result\n",
    "\n",
    "def aggregate(data):\n",
    "\n",
    "    # data.loc[:, 'sentiment'] = pd.Series(get_ft_label('\\n'.join(data.loc[:, 'text'].apply(lambda x: x.replace('\\n', '')))).split('\\n'), index=data.index).str[9].astype(int) - 1\n",
    "\n",
    "    parts_of_speech = reduce(lambda x, y: x.append(y, sort=True), data.loc[:, 'doc'].apply(lambda x: pd.DataFrame({k: [v] for k, v in dict(Counter([token.pos_ for token in x])).items()})))\n",
    "    parts_of_speech.index = data.index\n",
    "    parts_of_speech = parts_of_speech.fillna(0)\n",
    "    data = data.join(parts_of_speech)\n",
    "\n",
    "    tags = reduce(lambda x, y: x.append(y, sort=True), data.loc[:, 'doc'].apply(lambda x: pd.DataFrame({k: [v] for k, v in dict(Counter([token.tag_ for token in x])).items()})))\n",
    "    tags.index = data.index\n",
    "    tags = tags.fillna(0)\n",
    "    data = data.join(tags, rsuffix='_TAG')\n",
    "\n",
    "    both = reduce(lambda x, y: x.append(y, sort=True), data.loc[:, 'doc'].apply(lambda x: pd.DataFrame({k: [v] for k, v in dict(Counter([token.pos_ + ' ' + token.tag_ for token in x])).items()})))\n",
    "    both.index = data.index\n",
    "    both = both.fillna(0)\n",
    "    data = data.join(both, rsuffix='_BOTH')\n",
    "\n",
    "    features = reduce(lambda x, y: x.append(y, sort=True), data.loc[:, 'doc'].apply(lambda x: pd.DataFrame({k: [v] for k, v in dict(Counter([token._.feature for token in x if token._.feature])).items()})))\n",
    "    features.index = data.index\n",
    "    features = features.fillna(0)\n",
    "    data = data.join(features)\n",
    "\n",
    "    return data\n",
    "\n",
    "def process_data(data, process_text=True, apply_nlp=True, text_with_features=True, to_aggregate=True):\n",
    "\n",
    "    data = data.copy()\n",
    "\n",
    "    data.loc[:, 'text'] = data.loc[:, 'raw text'].str.replace(r'(\\.|\\,)\\w+', lambda x: '{} {}'.format(x.group(0)[0], x.group(0)[1:]), regex=True)\\\n",
    "                                                 .str.replace(r'(\\[(.*?)\\])|(\\((.*?)\\))', ' ', regex=True)\\\n",
    "                                                 .str.replace(r'\\[|\\]|\\(|\\)', ' ', regex=True)\\\n",
    "                                                 .str.replace(re.escape('..'), ' .SHORT ', regex=True)\\\n",
    "                                                 .str.replace(r'…', ' .LONG ', regex=True)\\\n",
    "                                                 .str.replace(r'^\\s*$', '', regex=True)\\\n",
    "                                                 .str.replace(r'\\n\\s*', '\\n', regex=True)\\\n",
    "                                                 .str.replace(r'(?<!\\w)((E|e)(M|m|H|h)+)|(U|u|M|m|H|h){2,}(?=\\s|,|\\.|\\-)', '.UMUH', regex=True)\\\n",
    "                                                 .str.replace(r'\\’', '\\'', regex=True)\\\n",
    "                                                 .str.replace(r'\\ +', ' ', regex=True)\\\n",
    "                                                 .str.replace(r'\\ +(?=\\n)', '', regex=True)\n",
    "\n",
    "    data.loc[data['text'].str.startswith('\\n'), 'text'] = data.loc[data['text'].str.startswith('\\n'), 'text'].str.replace(r'\\n+', '', 1, regex=True)\n",
    "\n",
    "    if process_text:\n",
    "        data.loc[:, 'processed text'] = data.loc[:, 'text'].str.replace(r'((\\w+)(-)(\\s*))+(\\w+)', lambda x: x.group(5) if x.group(2) in x.group(5) else x.group(2) + x.group(5) if x.group(2) + x.group(5) in WORD_SET else x.group(0), regex=True)\\\n",
    "                                                           .str.replace(r'(?<!\\w)((\\w)\\2+)(\\w+)', lambda x: x.group(2) + x.group(3) if x.group(2) + x.group(3) in WORD_SET else x.group(0), regex=True)\\\n",
    "                                                           .str.replace(r'(\\w+)(-)', '', regex=True)\\\n",
    "                                                           .str.replace(r'\\b((\\w|\\')+)((\\s|\\,|\\.)+\\1)+\\b', lambda x: x.group(1), regex=True)\\\n",
    "                                                           .str.replace(r'\\.[A-Z]+', '', regex=True)\\\n",
    "                                                           .str.replace(r'\\ +', ' ', regex=True)\\\n",
    "                                                           .str.replace(r'(?<![.])(?=[\\n\\r]|$)', '.', regex=True)\\\n",
    "                                                           .str.replace(r'(\\,|\\.)(\\s*(\\,|\\.))+', lambda x: x.group(1), regex=True)\\\n",
    "                                                           .str.replace(r'\\s+\\.', '.', regex=True)\n",
    "\n",
    "    if apply_nlp:\n",
    "        data.loc[:, 'doc'] = data.loc[:, 'processed text'].apply(nlp) if 'processed text' in data.columns else data.loc[:, 'text'].apply(nlp)\n",
    "        data.loc[:, 'doc'].apply(add_feature)\n",
    "\n",
    "        if text_with_features:\n",
    "            data.loc[:, 'text with features'] = data.loc[:, 'doc'].apply(get_text_with_features)\n",
    "\n",
    "        if to_aggregate:\n",
    "            data = aggregate(data)\n",
    "\n",
    "    return data\n",
    "\n",
    "def graph_pos(data, pidn, features=[], sentiment=False):\n",
    "\n",
    "    if features:\n",
    "        freq = data.loc[pidn, features].T.apply(lambda x: x / x.sum()).T\n",
    "    else:\n",
    "        freq = data.loc[pidn, 'ADJ':'VERB'].T.apply(lambda x: x / x.sum()).T\n",
    "\n",
    "    if sentiment:\n",
    "        fig, axes = plt.subplots(1, 2, gridspec_kw={'width_ratios': [9, 1]}, figsize=(12, 6))\n",
    "    else:\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(12, 6))\n",
    "        axes = [ax]\n",
    "\n",
    "    sns.barplot(x='variable',\n",
    "                y='value',\n",
    "                data=pd.melt(freq.reset_index(), id_vars='Number'),\n",
    "                ax=axes[0])\n",
    "\n",
    "    axes[0].set_xlabel('Part of Speech')\n",
    "    axes[0].set_ylabel('Frequency')\n",
    "\n",
    "    if sentiment:\n",
    "        sns.barplot(y=data.loc[pidn, 'sentiment'], ax=axes[1])\n",
    "\n",
    "    fig.suptitle('PIDN={}, n={}'.format(pidn, data.loc[pidn].shape[0]))\n",
    "\n",
    "    return fig\n",
    "\n",
    "def graph_features(data, variant, features=[], sentiment=False):\n",
    "    \n",
    "    data = data.loc[data['variant'] == variant]\n",
    "\n",
    "    if features:\n",
    "        freq = data.loc[:, features].T.apply(lambda x: x / x.sum()).T\n",
    "        figsize = (len(features), 4)\n",
    "        width_ratios = (len(features), 1)\n",
    "    else:\n",
    "        freq = data.loc[:, 'ADJ':'VERB'].T.apply(lambda x: x / x.sum()).T\n",
    "        figsize = (12, 4)\n",
    "        width_ratios = (12, 1)\n",
    "\n",
    "    if sentiment:\n",
    "        figsize = (figsize[0] + 4, figsize[1])\n",
    "        fig, axes = plt.subplots(1, 2, gridspec_kw={'width_ratios': width_ratios, 'wspace': 0.5}, figsize=figsize)\n",
    "    else:\n",
    "        fig, ax = plt.subplots(1, 1, figsize=figsize)\n",
    "        axes = [ax]\n",
    "\n",
    "    sns.barplot(x='variable',\n",
    "                y='value',\n",
    "                data=pd.melt(freq.reset_index(), id_vars='pidn'),\n",
    "                ax=axes[0])\n",
    "\n",
    "    axes[0].set_xlabel('Feature')\n",
    "    axes[0].set_ylabel('Frequency')\n",
    "    axes[0].set_ylim((0, 0.6))\n",
    "\n",
    "    if sentiment:\n",
    "        sns.barplot(y=data.loc[:, 'sentiment'], ax=axes[1])\n",
    "\n",
    "    fig.suptitle('variant={}, n={}'.format(variant, data.shape[0]))\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68de1fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dba6f3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c726c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2aea7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a1d98d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
