{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2cc376a5",
   "metadata": {},
   "source": [
    "### Date updated\n",
    "2024-01-04\n",
    "\n",
    "### Authors\n",
    "\n",
    "\n",
    "### Notes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41e1c110",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import shlex\n",
    "import subprocess\n",
    "from collections import Counter\n",
    "from functools import reduce\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from nltk.corpus import words\n",
    "\n",
    "#import docx\n",
    "import spacy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ebf3a8df-55eb-4e48-a2fd-3bbdd5fc832f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "spacy.prefer_gpu()\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89de2a69-a179-40af-9ea4-1788442afca4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.lang.en.English at 0x2c47da510>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp\n",
    "#import en_core_web_sm\n",
    "#nlp = en_core_web_sm.load()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "086519c1-4949-4461-a75f-eaadc73d9f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fb41ac2a-a0b2-4f3d-945d-e7f02f094f0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to /Users/reshamas/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/words.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "75699058",
   "metadata": {},
   "outputs": [],
   "source": [
    "#nlp = en_core_web_sm.load()\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "spacy.tokens.Token.set_extension('feature', default=None, force=True)\n",
    "\n",
    "WORD_SET = set(words.words())\n",
    "\n",
    "def read_data(PATH, print_exception=False):\n",
    "    data = pd.DataFrame(columns=['pidn', 'variant', 'text']).set_index('pidn')\n",
    "\n",
    "    for variant in os.listdir(PATH):\n",
    "        for fname in os.listdir(os.path.join(PATH, variant)):\n",
    "            try:\n",
    "                pidn = int(fname[:-4])\n",
    "                with open(os.path.join(os.path.join(PATH, variant), fname), 'r') as f:\n",
    "                    text = f.read()\n",
    "                data.loc[(pidn, 0), :] = variant, text\n",
    "            except Exception as e:\n",
    "                if print_exception:\n",
    "                    print(pidn, e)\n",
    "\n",
    "    data = data.sort_index()\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ec9a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "!which python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1e5b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda env list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9c53f123",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_docx(PATH, affix='', print_exception=False):\n",
    "    data = pd.DataFrame(columns=['pidn', 'visit', 'variant', 'raw text']).set_index(['pidn', 'visit'])\n",
    "\n",
    "    for variant in os.listdir(PATH):\n",
    "        if variant == '.DS_Store':\n",
    "            continue\n",
    "        for fname in os.listdir(os.path.join(PATH, variant + affix)):\n",
    "            try:\n",
    "                pidn = int(fname[:5])\n",
    "                if variant == 'long_nfvPPA':\n",
    "                    visit = 1\n",
    "                else:\n",
    "                    visit = 0\n",
    "                doc = docx.Document(os.path.join(os.path.join(PATH, variant + affix), fname))\n",
    "                text = '\\n'.join([x.text for x in doc.paragraphs if x.text != ''])\n",
    "                data.loc[(pidn ,visit), :] = variant, text\n",
    "            except Exception as e:\n",
    "                if print_exception:\n",
    "                    print(pidn, e)\n",
    "\n",
    "    data = data.sort_index()\n",
    "\n",
    "    return data\n",
    "\n",
    "def get_ft_label(text):\n",
    "\n",
    "    return subprocess.check_output(\"\"\"~/fastText/fasttext predict-prob ~/fastText/amazon_review_polarity.bin - <<< {}\"\"\".format(shlex.quote(text)), shell=True).decode('utf8')[:-1]\n",
    "\n",
    "# def aggregate(data):\n",
    "\n",
    "#     data.loc[:, 'sentiment'] = pd.Series(get_ft_label('\\n'.join(data.loc[:, 'text'].apply(lambda x: x.replace('\\n', '')))).split('\\n'), index=data.index).str[9].astype(int) - 1\n",
    "\n",
    "#     parts_of_speech = reduce(lambda x, y: x.append(y, sort=True), data.loc[:, 'doc'].apply(lambda x: pd.DataFrame({k: [v] for k, v in dict(Counter([token.pos_ for token in x])).items()})))\n",
    "#     parts_of_speech.index = data.index\n",
    "#     parts_of_speech = parts_of_speech.fillna(0)\n",
    "#     data = data.join(parts_of_speech)\n",
    "\n",
    "#     features = reduce(lambda x, y: x.append(y, sort=True), data.loc[:, 'doc'].apply(lambda x: pd.DataFrame({k: [v] for k, v in dict(Counter([token._.feature for token in x if token._.feature])).items()})))\n",
    "#     features.index = data.index\n",
    "#     features = features.fillna(0)\n",
    "#     data = data.join(features)\n",
    "\n",
    "#     return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e9802f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher1 = spacy.matcher.Matcher(nlp.vocab, validate=True)\n",
    "matcher1.add('there be', [[{'LOWER': 'there'}, {'POS': 'AUX'}]])\n",
    "matcher1.add('i see', [[{'LOWER': 'i'}, {'POS': 'VERB', 'LEMMA': 'see'}]])\n",
    "matcher1.add('not argument', [[{'LOWER': 'i'}, {'POS': 'VERB', 'LEMMA': 'see'}, {'DEP': {'IN': ['det', 'amod', 'advmod']}, 'OP': '*'}, {'DEP': 'dobj'}]])\n",
    "matcher1.add('appear to be', [[{'LEMMA': 'appear'}, {'LEMMA': 'to'}, {'LEMMA': 'be'}]])\n",
    "\n",
    "matcher2 = spacy.matcher.Matcher(nlp.vocab, validate=True)\n",
    "matcher2.add('aux', [[{'POS': 'AUX', 'DEP': {'IN': ['aux', 'auxpass']}}]])\n",
    "matcher2.add('copula', [[{'POS': 'AUX', 'DEP': {'NOT_IN': ['aux', 'auxpass']}}, {'POS': {'IN': ['ADV', 'PART']}, 'OP': '*'}, {'POS': {'NOT_IN': ['ADV', 'PART']}, 'DEP': {'NOT_IN': ['cc']}}]])\n",
    "matcher2.add('verb', [[{'POS': 'VERB'}]])\n",
    "matcher2.add('argument', [[{'DEP': 'dobj'}], [{'LOWER': 'i'}, {'POS': 'VERB', 'LEMMA': 'see'}, {'DEP': {'IN': ['det', 'amod', 'advmod']}, 'OP': '*'}, {'DEP': 'nsubj'}]])\n",
    "matcher2.add('predicate', [[{'POS': 'AUX'}, {'DEP': {'IN': ['det', 'amod', 'advmod']}, 'OP': '*'}, {'DEP': 'attr'}]])\n",
    "matcher2.add('adjunct', [[{'DEP': 'pobj'}]])\n",
    "\n",
    "# def add_feature(doc):\n",
    "\n",
    "#     for x in matcher1(doc):\n",
    "#         if nlp.vocab.strings[x[0]] in ('there be', 'i see', 'not argument'):\n",
    "#             doc[x[1]:x[2]][-1]._.feature = nlp.vocab.strings[x[0]]\n",
    "#         elif not doc[x[1]:x[2]][0]._.feature:\n",
    "#             doc[x[1]:x[2]][0]._.feature = nlp.vocab.strings[x[0]]\n",
    "\n",
    "#     for x in matcher2(doc):\n",
    "#         if nlp.vocab.strings[x[0]] in ('predicate', 'argument') and not doc[x[1]:x[2]][-1]._.feature:\n",
    "#             doc[x[1]:x[2]][-1]._.feature = nlp.vocab.strings[x[0]]\n",
    "#         elif not doc[x[1]:x[2]][0]._.feature:\n",
    "#             doc[x[1]:x[2]][0]._.feature = nlp.vocab.strings[x[0]]\n",
    "\n",
    "def add_feature(doc):\n",
    "\n",
    "    def visit(word):\n",
    "\n",
    "        if word.pos_ == 'AUX':\n",
    "            if word.dep_ == 'aux':\n",
    "                word._.feature = 'aux' # Syntactic auxiliary verb\n",
    "            elif word.n_lefts > 0 and next(word.lefts).lemma_ == 'there':\n",
    "                word._.feature = 'there be'\n",
    "            elif word.n_rights > 0 and next(word.rights).dep_ != 'cc':\n",
    "                word._.feature = 'copula'\n",
    "            else:\n",
    "                word._.feature = 'other aux'\n",
    "        elif word.pos_ == 'VERB':\n",
    "            if word.lemma_ == 'see' and word.n_lefts > 0 and next(word.lefts).text.upper() == 'I':\n",
    "                word._.feature = 'i see'\n",
    "            elif word.lemma_ != 'see' and word.n_lefts > 0 and next(word.lefts).text.upper() == 'I':\n",
    "                word._.feature = 'i ...'\n",
    "            elif word.lemma_ == 'appear' and word.n_rights > 0 and next(word.rights).pos_ == 'AUX':\n",
    "                word._.feature = 'appear to be'\n",
    "            elif word.lemma_ == 'look' and word.n_rights > 0 and next(word.rights).pos_ == 'SCONJ':\n",
    "                word._.feature = 'look like'\n",
    "            elif word.dep_ == 'ccomp':\n",
    "                word._.feature = 'verb'\n",
    "            else:\n",
    "                word._.feature = 'verb'\n",
    "        elif word.pos_ == 'NOUN' or word.pos_ == 'NUM':\n",
    "            try:\n",
    "                anc = next(word.ancestors)\n",
    "                if anc.pos_ == 'VERB' and word.dep_ == 'dobj':\n",
    "                    if anc._.feature and anc._.feature == 'i see':\n",
    "                        word._.feature = 'predicate'\n",
    "                    else:\n",
    "                        word._.feature = 'argument'\n",
    "                elif anc.pos_ == 'ADP' and word.dep_ == 'pobj':\n",
    "                    if anc.text.lower() != 'of':\n",
    "                        try:\n",
    "                            anc_anc = next(anc.ancestors)\n",
    "                            if anc_anc.pos_ == 'AUX' and len([x for x in anc_anc.rights if x.pos_ == 'ADP']) == 1:\n",
    "                                word._.feature = 'argument'\n",
    "                            else:\n",
    "                                word._.feature = 'adjunct'\n",
    "                        except:\n",
    "                            word._.feature = 'adjunct'\n",
    "                    else:\n",
    "                        try:\n",
    "                            anc_anc = next(anc.ancestors)\n",
    "                            if anc_anc._.feature:\n",
    "                                word._.feature = anc_anc._.feature\n",
    "                                anc_anc._.feature = '... of'\n",
    "                        except:\n",
    "                            pass\n",
    "                elif anc.pos_ == 'AUX' and word.dep_ == 'attr' or word.dep_ == 'dobj':\n",
    "                    word._.feature = 'predicate'\n",
    "                elif anc.pos_ == 'SCONJ' and word.dep_ == 'pobj':\n",
    "                    try:\n",
    "                        anc_anc = next(anc.ancestors)\n",
    "                        if anc_anc.pos_ == 'AUX' or anc_anc.lemma_ == 'look':\n",
    "                            word._.feature = 'predicate'\n",
    "                        else:\n",
    "                            word._.feature = 'adjunct'\n",
    "                    except:\n",
    "                        pass\n",
    "                elif anc._.feature and 'conj' not in anc._.feature and word.dep_ == 'conj':\n",
    "                    if anc.pos_ != 'VERB':\n",
    "                        word._.feature = anc._.feature + ' conj'\n",
    "                    else:\n",
    "                        try:\n",
    "                            anc_anc = next(anc.ancestors)\n",
    "                            if anc_anc._.feature and anc_anc.pos_ != 'VERB':\n",
    "                                word._.feature = anc_anc._.feature + ' conj'\n",
    "                        except:\n",
    "                            pass\n",
    "            except:\n",
    "                pass\n",
    "        elif word.pos_ == 'ADV' and word.dep_ == 'advmod':\n",
    "            try:\n",
    "                anc = next(word.ancestors)\n",
    "                if anc.pos_ not in ('ADV', 'AUX'):\n",
    "                    word._.feature = 'adv adjunct'\n",
    "                elif anc.pos_ == 'AUX':\n",
    "                    n_nouns = 0\n",
    "                    n_advs = 0\n",
    "                    for right in anc.rights:\n",
    "                        if right.pos_ == 'NOUN':\n",
    "                            n_nouns += 1\n",
    "                        elif right.pos_ == 'ADV':\n",
    "                            n_advs += 1\n",
    "                    if n_nouns > 0:\n",
    "                        word._.feature = 'adv adjunct'\n",
    "                    elif n_advs == 1:\n",
    "                        word._.feature = 'predicate'\n",
    "            except:\n",
    "                pass\n",
    "        elif word.pos_ == 'ADJ' and word.dep_ == 'acomp':\n",
    "            word._.feature = 'predicate'\n",
    "\n",
    "        if word._.feature == 'adjunct':\n",
    "            try:\n",
    "                anc = next(word.ancestors)\n",
    "                if anc.lemma_ == 'into':\n",
    "                    try:\n",
    "                        anc_anc = next(anc.ancestors)\n",
    "                        if anc_anc.lemma_ == 'pour':\n",
    "                            word._.feature = 'argument'\n",
    "                    except:\n",
    "                        pass\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        for child in word.children:\n",
    "            visit(child)\n",
    "\n",
    "\n",
    "    for sent in doc.sents:\n",
    "        if len(sent) > 0:\n",
    "            visit(sent.root)\n",
    "\n",
    "def get_text_with_features(doc):\n",
    "\n",
    "    def join_punctuation(seq, characters='.,;?!'):\n",
    "        characters = set(characters)\n",
    "        seq = iter(seq)\n",
    "        current = next(seq)\n",
    "\n",
    "        for nxt in seq:\n",
    "            if nxt in characters:\n",
    "                current += nxt\n",
    "            else:\n",
    "                yield current\n",
    "                current = nxt\n",
    "\n",
    "        yield current\n",
    "\n",
    "    result = ' '.join(join_punctuation([token.text + ('<{}>'.format(token._.feature) if token._.feature else '') for token in doc]))\n",
    "    result = re.sub(r'\\n\\ +', '\\n', result)\n",
    "    return result\n",
    "\n",
    "def get_text_with_pos(doc, pos='pos'):\n",
    "\n",
    "    def join_punctuation(seq, characters='.,;?!'):\n",
    "        characters = set(characters)\n",
    "        seq = iter(seq)\n",
    "        current = next(seq)\n",
    "\n",
    "        for nxt in seq:\n",
    "            if nxt in characters:\n",
    "                current += nxt\n",
    "            else:\n",
    "                yield current\n",
    "                current = nxt\n",
    "\n",
    "        yield current\n",
    "\n",
    "    result = ' '.join(join_punctuation([token.text + ('<{}>'.format(token.pos_ if pos == 'pos' else token.tag_ if pos == 'tag' else token.pos_ + ' ' + token.tag_) if not token.pos_ in ('PUNCT', 'SPACE')  else '') for token in doc]))\n",
    "    result = re.sub(r'\\n\\ +', '\\n', result)\n",
    "    return result\n",
    "\n",
    "def aggregate(data):\n",
    "\n",
    "    # data.loc[:, 'sentiment'] = pd.Series(get_ft_label('\\n'.join(data.loc[:, 'text'].apply(lambda x: x.replace('\\n', '')))).split('\\n'), index=data.index).str[9].astype(int) - 1\n",
    "\n",
    "    parts_of_speech = reduce(lambda x, y: x.append(y, sort=True), data.loc[:, 'doc'].apply(lambda x: pd.DataFrame({k: [v] for k, v in dict(Counter([token.pos_ for token in x])).items()})))\n",
    "    parts_of_speech.index = data.index\n",
    "    parts_of_speech = parts_of_speech.fillna(0)\n",
    "    data = data.join(parts_of_speech)\n",
    "\n",
    "    tags = reduce(lambda x, y: x.append(y, sort=True), data.loc[:, 'doc'].apply(lambda x: pd.DataFrame({k: [v] for k, v in dict(Counter([token.tag_ for token in x])).items()})))\n",
    "    tags.index = data.index\n",
    "    tags = tags.fillna(0)\n",
    "    data = data.join(tags, rsuffix='_TAG')\n",
    "\n",
    "    both = reduce(lambda x, y: x.append(y, sort=True), data.loc[:, 'doc'].apply(lambda x: pd.DataFrame({k: [v] for k, v in dict(Counter([token.pos_ + ' ' + token.tag_ for token in x])).items()})))\n",
    "    both.index = data.index\n",
    "    both = both.fillna(0)\n",
    "    data = data.join(both, rsuffix='_BOTH')\n",
    "\n",
    "    features = reduce(lambda x, y: x.append(y, sort=True), data.loc[:, 'doc'].apply(lambda x: pd.DataFrame({k: [v] for k, v in dict(Counter([token._.feature for token in x if token._.feature])).items()})))\n",
    "    features.index = data.index\n",
    "    features = features.fillna(0)\n",
    "    data = data.join(features)\n",
    "\n",
    "    return data\n",
    "\n",
    "def process_data(data, process_text=True, apply_nlp=True, text_with_features=True, to_aggregate=True):\n",
    "\n",
    "    data = data.copy()\n",
    "\n",
    "    data.loc[:, 'text'] = data.loc[:, 'raw text'].str.replace(r'(\\.|\\,)\\w+', lambda x: '{} {}'.format(x.group(0)[0], x.group(0)[1:]), regex=True)\\\n",
    "                                                 .str.replace(r'(\\[(.*?)\\])|(\\((.*?)\\))', ' ', regex=True)\\\n",
    "                                                 .str.replace(r'\\[|\\]|\\(|\\)', ' ', regex=True)\\\n",
    "                                                 .str.replace(re.escape('..'), ' .SHORT ', regex=True)\\\n",
    "                                                 .str.replace(r'…', ' .LONG ', regex=True)\\\n",
    "                                                 .str.replace(r'^\\s*$', '', regex=True)\\\n",
    "                                                 .str.replace(r'\\n\\s*', '\\n', regex=True)\\\n",
    "                                                 .str.replace(r'(?<!\\w)((E|e)(M|m|H|h)+)|(U|u|M|m|H|h){2,}(?=\\s|,|\\.|\\-)', '.UMUH', regex=True)\\\n",
    "                                                 .str.replace(r'\\’', '\\'', regex=True)\\\n",
    "                                                 .str.replace(r'\\ +', ' ', regex=True)\\\n",
    "                                                 .str.replace(r'\\ +(?=\\n)', '', regex=True)\n",
    "\n",
    "    data.loc[data['text'].str.startswith('\\n'), 'text'] = data.loc[data['text'].str.startswith('\\n'), 'text'].str.replace(r'\\n+', '', 1, regex=True)\n",
    "\n",
    "    if process_text:\n",
    "        data.loc[:, 'processed text'] = data.loc[:, 'text'].str.replace(r'((\\w+)(-)(\\s*))+(\\w+)', lambda x: x.group(5) if x.group(2) in x.group(5) else x.group(2) + x.group(5) if x.group(2) + x.group(5) in WORD_SET else x.group(0), regex=True)\\\n",
    "                                                           .str.replace(r'(?<!\\w)((\\w)\\2+)(\\w+)', lambda x: x.group(2) + x.group(3) if x.group(2) + x.group(3) in WORD_SET else x.group(0), regex=True)\\\n",
    "                                                           .str.replace(r'(\\w+)(-)', '', regex=True)\\\n",
    "                                                           .str.replace(r'\\b((\\w|\\')+)((\\s|\\,|\\.)+\\1)+\\b', lambda x: x.group(1), regex=True)\\\n",
    "                                                           .str.replace(r'\\.[A-Z]+', '', regex=True)\\\n",
    "                                                           .str.replace(r'\\ +', ' ', regex=True)\\\n",
    "                                                           .str.replace(r'(?<![.])(?=[\\n\\r]|$)', '.', regex=True)\\\n",
    "                                                           .str.replace(r'(\\,|\\.)(\\s*(\\,|\\.))+', lambda x: x.group(1), regex=True)\\\n",
    "                                                           .str.replace(r'\\s+\\.', '.', regex=True)\n",
    "\n",
    "    if apply_nlp:\n",
    "        data.loc[:, 'doc'] = data.loc[:, 'processed text'].apply(nlp) if 'processed text' in data.columns else data.loc[:, 'text'].apply(nlp)\n",
    "        data.loc[:, 'doc'].apply(add_feature)\n",
    "\n",
    "        if text_with_features:\n",
    "            data.loc[:, 'text with features'] = data.loc[:, 'doc'].apply(get_text_with_features)\n",
    "\n",
    "        if to_aggregate:\n",
    "            data = aggregate(data)\n",
    "\n",
    "    return data\n",
    "\n",
    "def graph_pos(data, pidn, features=[], sentiment=False):\n",
    "\n",
    "    if features:\n",
    "        freq = data.loc[pidn, features].T.apply(lambda x: x / x.sum()).T\n",
    "    else:\n",
    "        freq = data.loc[pidn, 'ADJ':'VERB'].T.apply(lambda x: x / x.sum()).T\n",
    "\n",
    "    if sentiment:\n",
    "        fig, axes = plt.subplots(1, 2, gridspec_kw={'width_ratios': [9, 1]}, figsize=(12, 6))\n",
    "    else:\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(12, 6))\n",
    "        axes = [ax]\n",
    "\n",
    "    sns.barplot(x='variable',\n",
    "                y='value',\n",
    "                data=pd.melt(freq.reset_index(), id_vars='Number'),\n",
    "                ax=axes[0])\n",
    "\n",
    "    axes[0].set_xlabel('Part of Speech')\n",
    "    axes[0].set_ylabel('Frequency')\n",
    "\n",
    "    if sentiment:\n",
    "        sns.barplot(y=data.loc[pidn, 'sentiment'], ax=axes[1])\n",
    "\n",
    "    fig.suptitle('PIDN={}, n={}'.format(pidn, data.loc[pidn].shape[0]))\n",
    "\n",
    "    return fig\n",
    "\n",
    "def graph_features(data, variant, features=[], sentiment=False):\n",
    "    \n",
    "    data = data.loc[data['variant'] == variant]\n",
    "\n",
    "    if features:\n",
    "        freq = data.loc[:, features].T.apply(lambda x: x / x.sum()).T\n",
    "        figsize = (len(features), 4)\n",
    "        width_ratios = (len(features), 1)\n",
    "    else:\n",
    "        freq = data.loc[:, 'ADJ':'VERB'].T.apply(lambda x: x / x.sum()).T\n",
    "        figsize = (12, 4)\n",
    "        width_ratios = (12, 1)\n",
    "\n",
    "    if sentiment:\n",
    "        figsize = (figsize[0] + 4, figsize[1])\n",
    "        fig, axes = plt.subplots(1, 2, gridspec_kw={'width_ratios': width_ratios, 'wspace': 0.5}, figsize=figsize)\n",
    "    else:\n",
    "        fig, ax = plt.subplots(1, 1, figsize=figsize)\n",
    "        axes = [ax]\n",
    "\n",
    "    sns.barplot(x='variable',\n",
    "                y='value',\n",
    "                data=pd.melt(freq.reset_index(), id_vars='pidn'),\n",
    "                ax=axes[0])\n",
    "\n",
    "    axes[0].set_xlabel('Feature')\n",
    "    axes[0].set_ylabel('Frequency')\n",
    "    axes[0].set_ylim((0, 0.6))\n",
    "\n",
    "    if sentiment:\n",
    "        sns.barplot(y=data.loc[:, 'sentiment'], ax=axes[1])\n",
    "\n",
    "    fig.suptitle('variant={}, n={}'.format(variant, data.shape[0]))\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68de1fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dba6f3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c726c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2aea7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a1d98d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
