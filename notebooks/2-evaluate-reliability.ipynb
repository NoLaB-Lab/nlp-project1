{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2cc376a5",
   "metadata": {},
   "source": [
    "### Info\n",
    "- Date: 2024-05-01\n",
    "- Author: Reshama S\n",
    "- Location: https://github.com/NoLaB-Lab/nlp-project1\n",
    "\n",
    "### Description\n",
    "- Evalute human vs ai transcripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c4e6991-bede-48bb-bccf-a99321aa4a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import pprint\n",
    "from Levenshtein import distance\n",
    "from Levenshtein import ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e0bd6c5-7ad2-4571-8286-fe8201ddf62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_human = \"../data/transcripts-clinician/\"\n",
    "dir_ai = \"../data/transcripts-whisper/\"\n",
    "\n",
    "dict_scores = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0180da8d-c06b-4ce1-a997-65d756790bfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AJ_IMG_3334\n",
      "{'rouge1': 0.912751677852349, 'rouge2': 0.8224719101123596, 'rougeL': 0.9038031319910516, 'rougeLsum': 0.9038031319910516}\n",
      "Levenshtein disagreement: 156\n",
      "Levensshtein ratio: 0.9153952843273232\n",
      "Levenshtein distance: 156\n",
      "Normalized Levenshtein distance: 0.14156079854809436\n",
      "--------------------------------------------------\n",
      "AJ_IMG_3335\n",
      "{'rouge1': 0.8080229226361031, 'rouge2': 0.7146974063400576, 'rougeL': 0.8080229226361031, 'rougeLsum': 0.8080229226361031}\n",
      "Levenshtein disagreement: 303\n",
      "Levensshtein ratio: 0.8016928657799275\n",
      "Levenshtein distance: 303\n",
      "Normalized Levenshtein distance: 0.3159541188738269\n",
      "--------------------------------------------------\n",
      "AP_IMG_3383\n",
      "{'rouge1': 0.8942307692307694, 'rouge2': 0.8019323671497584, 'rougeL': 0.8749999999999999, 'rougeLsum': 0.8894230769230769}\n",
      "Levenshtein disagreement: 226\n",
      "Levensshtein ratio: 0.8469493278179938\n",
      "Levenshtein distance: 226\n",
      "Normalized Levenshtein distance: 0.22944162436548224\n",
      "--------------------------------------------------\n",
      "AP_IMG_3384\n",
      "{'rouge1': 0.8087167070217919, 'rouge2': 0.6763990267639904, 'rougeL': 0.7796610169491525, 'rougeLsum': 0.8038740920096852}\n",
      "Levenshtein disagreement: 379\n",
      "Levensshtein ratio: 0.7931034482758621\n",
      "Levenshtein distance: 379\n",
      "Normalized Levenshtein distance: 0.31322314049586775\n",
      "--------------------------------------------------\n",
      "BM_IMG_3480\n",
      "{'rouge1': 0.8449367088607596, 'rouge2': 0.6952380952380952, 'rougeL': 0.800632911392405, 'rougeLsum': 0.8259493670886077}\n",
      "Levenshtein disagreement: 435\n",
      "Levensshtein ratio: 0.8181818181818181\n",
      "Levenshtein distance: 435\n",
      "Normalized Levenshtein distance: 0.300414364640884\n",
      "--------------------------------------------------\n",
      "BM_IMG_3481\n",
      "{'rouge1': 0.7617328519855596, 'rouge2': 0.6014492753623187, 'rougeL': 0.707581227436823, 'rougeLsum': 0.7545126353790613}\n",
      "Levenshtein disagreement: 470\n",
      "Levensshtein ratio: 0.7626281807823775\n",
      "Levenshtein distance: 470\n",
      "Normalized Levenshtein distance: 0.34306569343065696\n",
      "--------------------------------------------------\n",
      "MW_IMG_3200\n",
      "{'rouge1': 0.7508650519031141, 'rouge2': 0.5833333333333335, 'rougeL': 0.7024221453287197, 'rougeLsum': 0.7404844290657439}\n",
      "Levenshtein disagreement: 539\n",
      "Levensshtein ratio: 0.7675407512402551\n",
      "Levenshtein distance: 539\n",
      "Normalized Levenshtein distance: 0.37352737352737353\n",
      "--------------------------------------------------\n",
      "MW_IMG_3201\n",
      "{'rouge1': 0.8780487804878048, 'rouge2': 0.7459016393442623, 'rougeL': 0.8536585365853658, 'rougeLsum': 0.8536585365853658}\n",
      "Levenshtein disagreement: 117\n",
      "Levensshtein ratio: 0.8741379310344828\n",
      "Levenshtein distance: 117\n",
      "Normalized Levenshtein distance: 0.19931856899488926\n",
      "--------------------------------------------------\n",
      "PG_IMG_3189\n",
      "{'rouge1': 0.81875, 'rouge2': 0.6269592476489029, 'rougeL': 0.753125, 'rougeLsum': 0.7875}\n",
      "Levenshtein disagreement: 513\n",
      "Levensshtein ratio: 0.7948463825569871\n",
      "Levenshtein distance: 513\n",
      "Normalized Levenshtein distance: 0.3114754098360656\n",
      "--------------------------------------------------\n",
      "PG_IMG_3190\n",
      "{'rouge1': 0.8611898016997167, 'rouge2': 0.7635327635327634, 'rougeL': 0.8498583569405099, 'rougeLsum': 0.8498583569405099}\n",
      "Levenshtein disagreement: 190\n",
      "Levensshtein ratio: 0.8547008547008547\n",
      "Levenshtein distance: 190\n",
      "Normalized Levenshtein distance: 0.22170361726954493\n",
      "--------------------------------------------------\n",
      "RF_IMG_3240\n",
      "{'rouge1': 0.89419795221843, 'rouge2': 0.7697594501718212, 'rougeL': 0.8668941979522184, 'rougeLsum': 0.8873720136518771}\n",
      "Levenshtein disagreement: 178\n",
      "Levensshtein ratio: 0.8474341192787795\n",
      "Levenshtein distance: 178\n",
      "Normalized Levenshtein distance: 0.23670212765957446\n",
      "--------------------------------------------------\n",
      "RF_IMG_3241\n",
      "{'rouge1': 0.9210526315789473, 'rouge2': 0.847682119205298, 'rougeL': 0.9210526315789473, 'rougeLsum': 0.9144736842105264}\n",
      "Levenshtein disagreement: 81\n",
      "Levensshtein ratio: 0.9309878213802436\n",
      "Levenshtein distance: 81\n",
      "Normalized Levenshtein distance: 0.10857908847184987\n",
      "--------------------------------------------------\n",
      "SS_IMG_2862\n",
      "{'rouge1': 0.9158415841584159, 'rouge2': 0.791044776119403, 'rougeL': 0.8811881188118812, 'rougeLsum': 0.9009900990099011}\n",
      "Levenshtein disagreement: 182\n",
      "Levensshtein ratio: 0.9004329004329005\n",
      "Levenshtein distance: 182\n",
      "Normalized Levenshtein distance: 0.1720226843100189\n",
      "--------------------------------------------------\n",
      "SS_IMG_2863\n",
      "{'rouge1': 0.7400881057268723, 'rouge2': 0.5866666666666667, 'rougeL': 0.7312775330396477, 'rougeLsum': 0.7312775330396477}\n",
      "Levenshtein disagreement: 208\n",
      "Levensshtein ratio: 0.7803521779425394\n",
      "Levenshtein distance: 208\n",
      "Normalized Levenshtein distance: 0.3382113821138211\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def evaltext(filename):\n",
    "    print(filename)\n",
    "    file = open(dir_human + filename + \".txt\", \"r\")\n",
    "    content_human = file.read()\n",
    "    file.close()\n",
    "    #print(content_human)\n",
    "    #print(\"-\" * 50)\n",
    "    \n",
    "    file_ai = open(dir_ai + filename + \".txt\", \"r\")\n",
    "    content_ai = file_ai.read()\n",
    "    file_ai.close()\n",
    "    #print(content_ai)\n",
    "    #print(\"-\" * 50)\n",
    "\n",
    "    # load the metric (from Hugging Face)\n",
    "    score = evaluate.load('rouge')\n",
    "    #score = evaluate.load(\"accuracy\") # this gives error\n",
    "\n",
    "    results = score.compute(predictions=[content_ai],\n",
    "                         references=[content_human])\n",
    "    print(results)\n",
    "    dict_scores[filename] = results\n",
    "\n",
    "    disagreement = distance(content_human, content_ai)\n",
    "    print(f\"Levenshtein disagreement: {disagreement}\")\n",
    "    \n",
    "    ratiov = ratio(content_human, content_ai)\n",
    "    print(f\"Levensshtein ratio: {ratiov}\")\n",
    "\n",
    "    # Calculate normalized distance (between 0 and 1)\n",
    "    levenshtein_distance = distance(content_human, content_ai)\n",
    "    print(f\"Levenshtein distance: {levenshtein_distance}\")\n",
    "    sentence_length = max(len(content_human), len(content_ai))\n",
    "    normalized_distance = levenshtein_distance / sentence_length\n",
    "\n",
    "    print(f\"Normalized Levenshtein distance: {normalized_distance}\")\n",
    "\n",
    "    print('-' * 50)\n",
    "    \n",
    "evaltext(\"AJ_IMG_3334\")\n",
    "evaltext(\"AJ_IMG_3335\")\n",
    "evaltext(\"AP_IMG_3383\")\n",
    "evaltext(\"AP_IMG_3384\")\n",
    "evaltext(\"BM_IMG_3480\")\n",
    "evaltext(\"BM_IMG_3481\")\n",
    "evaltext(\"MW_IMG_3200\")\n",
    "evaltext(\"MW_IMG_3201\")\n",
    "evaltext(\"PG_IMG_3189\")\n",
    "evaltext(\"PG_IMG_3190\")\n",
    "evaltext(\"RF_IMG_3240\")\n",
    "evaltext(\"RF_IMG_3241\")\n",
    "evaltext(\"SS_IMG_2862\")\n",
    "evaltext(\"SS_IMG_2863\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6637ad70-fa59-4b01-b43b-6cf7cbdc2403",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f03a541e-f9aa-4170-83f0-a85e4f103a5b",
   "metadata": {},
   "source": [
    "### ROUGE score\n",
    "- A ROUGE score close to zero indicates poor similarity between candidate and references. \n",
    "- A ROUGE score close to one indicates strong similarity between candidate and references. \n",
    "- If candidate is identical to one of the reference documents, then score is 1.\n",
    "\n",
    "### Levenshtein score\n",
    "https://rapidfuzz.github.io/Levenshtein/levenshtein.html#distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5383e1-16b5-437c-bb67-47dd8e2c2cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dict_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0c5711-83ca-4ac7-a532-078dd32dc2d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82c74dec-5f5c-4267-a83f-f7dbc99d7501",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ref: https://towardsdatascience.com/side-by-side-comparison-of-strings-in-python-b9491ac858\n",
    "\n",
    "import difflib\n",
    "import re\n",
    "\n",
    "def tokenize(s):\n",
    "    return re.split('\\s+', s)\n",
    "def untokenize(ts):\n",
    "    return ' '.join(ts)\n",
    "        \n",
    "def equalize(s1, s2):\n",
    "    l1 = tokenize(s1)\n",
    "    l2 = tokenize(s2)\n",
    "    res1 = []\n",
    "    res2 = []\n",
    "    prev = difflib.Match(0,0,0)\n",
    "    for match in difflib.SequenceMatcher(a=l1, b=l2).get_matching_blocks():\n",
    "        if (prev.a + prev.size != match.a):\n",
    "            for i in range(prev.a + prev.size, match.a):\n",
    "                res2 += ['_' * len(l1[i])]\n",
    "            res1 += l1[prev.a + prev.size:match.a]\n",
    "        if (prev.b + prev.size != match.b):\n",
    "            for i in range(prev.b + prev.size, match.b):\n",
    "                res1 += ['_' * len(l2[i])]\n",
    "            res2 += l2[prev.b + prev.size:match.b]\n",
    "        res1 += l1[match.a:match.a+match.size]\n",
    "        res2 += l2[match.b:match.b+match.size]\n",
    "        prev = match\n",
    "    return untokenize(res1), untokenize(res2)\n",
    "\n",
    "def insert_newlines(string, every=64, window=10):\n",
    "    result = []\n",
    "    from_string = string\n",
    "    while len(from_string) > 0:\n",
    "        cut_off = every\n",
    "        if len(from_string) > every:\n",
    "            while (from_string[cut_off-1] != ' ') and (cut_off > (every-window)):\n",
    "                cut_off -= 1\n",
    "        else:\n",
    "            cut_off = len(from_string)\n",
    "        part = from_string[:cut_off]\n",
    "        result += [part]\n",
    "        from_string = from_string[cut_off:]\n",
    "    return result\n",
    "\n",
    "def show_comparison(s1, s2, width=40, margin=10, sidebyside=True, compact=False):\n",
    "    s1, s2 = equalize(s1,s2)\n",
    "\n",
    "    if sidebyside:\n",
    "        s1 = insert_newlines(s1, width, margin)\n",
    "        s2 = insert_newlines(s2, width, margin)\n",
    "        if compact:\n",
    "            for i in range(0, len(s1)):\n",
    "                lft = re.sub(' +', ' ', s1[i].replace('_', '')).ljust(width)\n",
    "                rgt = re.sub(' +', ' ', s2[i].replace('_', '')).ljust(width) \n",
    "                print(lft + ' | ' + rgt + ' | ')        \n",
    "        else:\n",
    "            for i in range(0, len(s1)):\n",
    "                lft = s1[i].ljust(width)\n",
    "                rgt = s2[i].ljust(width)\n",
    "                print(lft + ' | ' + rgt + ' | ')\n",
    "    else:\n",
    "        print(s1)\n",
    "        print(s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "940e4216-b341-4bd7-a8d7-3f95fd90083a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def comptext(filename):\n",
    "    print(filename)\n",
    "    file = open(dir_human + filename + \".txt\", \"r\")\n",
    "    content_human = file.read()\n",
    "    file.close()\n",
    "    #print(content_human)\n",
    "    #print(\"-\" * 50)\n",
    "    \n",
    "    file_ai = open(dir_ai + filename + \".txt\", \"r\")\n",
    "    content_ai = file_ai.read()\n",
    "    file_ai.close()\n",
    "    #print(content_ai)\n",
    "    #print(\"-\" * 50)\n",
    "\n",
    "    print('-' * 50)\n",
    "    show_comparison(content_human, content_ai, width=50, sidebyside=True, compact=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b29a5f76-e859-4018-84f1-5f4313565c7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF_IMG_3241\n",
      "{'rouge1': 0.9210526315789473, 'rouge2': 0.847682119205298, 'rougeL': 0.9210526315789473, 'rougeLsum': 0.9144736842105264}\n",
      "Levenshtein disagreement: 81\n",
      "Levensshtein ratio: 0.9309878213802436\n",
      "Levenshtein distance: 81\n",
      "Normalized Levenshtein distance: 0.10857908847184987\n",
      "--------------------------------------------------\n",
      "RF_IMG_3241\n",
      "--------------------------------------------------\n",
      " I'm also going to ask you to tell me a story      |  I'm also going to ask you to tell me a story      | \n",
      "about a bad, hopefully not too traumatic, but      | about a bad, hopefully not too traumatic, but      | \n",
      "like a bad childhood memory _______ if you have    | like a bad childhood ______ memory, if you have    | \n",
      "any ____ or adolescent ____________ something      | ___ any, or __________ adolescence, something      | \n",
      "that happened that wasn't too great ______ ___ I   | that happened that wasn't ___ _____ great. Oh, I   | \n",
      "remember when I was very young. well ______ _____  | remember when I was very ______ ____ young, well,  | \n",
      "not very _____ maybe about 10 years old ____ my    | not ____ very, maybe about 10 years ___ old, my    | \n",
      "father worked in a summer camp and a few of us     | father worked in a summer camp and a few of us     | \n",
      "kids were horsing around and we were climbing out  | kids were horsing around and we were climbing out  | \n",
      "of a window and I sort of fell and hit my wrist    | of a window and I sort of fell and hit my wrist    | \n",
      "and broke my wrist which ______ _____ made my the  | and broke my _____ _____ wrist. Which made __ the  | \n",
      "rest of the summer this _______ _ was in the       | rest of the ______ ____ summer, I was in the       | \n",
      "beginning of the summer _______ not too much fun   | beginning of the ______ summer, not too much fun   | \n",
      "cause _______ I had a cast on my wrist for like    | _____ because I had a cast on my wrist for like    | \n",
      "three-quarters _____ ________ of the summr oh      | ______________ three quarters of the _____ __      | \n",
      "_______ __ my goodness did _________ ___ your      | summer. Oh my ________ ___ goodness. Did your      | \n",
      "dad, did you get in trouble for it or yeah ___     | dad, did you get in trouble for __ __ ____ it?     | \n",
      "_____ ______ minor minor trouble yeah lets see     | Yeah, minor, minor _____ _______ ____ ____ ___     | \n",
      "thats okay , thats fine  ________ ______ _____     | _____ ____ _ _____ ____  trouble. Yikes, let's     | \n",
      "____ ____ _____ ______ _____ ________              | see. It's okay, that's fine. Alright.              | \n"
     ]
    }
   ],
   "source": [
    "# highest score\n",
    "evaltext(\"RF_IMG_3241\")\n",
    "comptext(\"RF_IMG_3241\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e8744ba8-2767-4bfd-a78a-803d6da5ad0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SS_IMG_2863\n",
      "{'rouge1': 0.7400881057268723, 'rouge2': 0.5866666666666667, 'rougeL': 0.7312775330396477, 'rougeLsum': 0.7312775330396477}\n",
      "Levenshtein disagreement: 208\n",
      "Levensshtein ratio: 0.7803521779425394\n",
      "Levenshtein distance: 208\n",
      "Normalized Levenshtein distance: 0.3382113821138211\n",
      "--------------------------------------------------\n",
      "SS_IMG_2863\n",
      "--------------------------------------------------\n",
      "Now One  ___ ___ more thing I'm going ____ ___     | ___ ___  And one more thing ___ _____ I'll ask     | \n",
      "___ to have you tell me _____ is a childhood       | you to ____ ___ tell me about is a childhood       | \n",
      "memory that wasn't good, that was sad, ____ _ ___  | memory that wasn't good, ____ ___ ____ like a sad  | \n",
      "___ ___ _____ hopefully not too dramatic Yeah      | or, you know, hopefully not too ________ ____      | \n",
      "__________ ___ ____ _ ___ ______ ____ __________   | traumatic, but like a bad memory from childhood.   | \n",
      "_____ I have ___ _ ______ fairly good, you know,   | Yeah. I ____ had a pretty fairly good, you know,   | \n",
      "no issues that I can really remeber _________ but  | no issues that I can really _______ remember, but  | \n",
      "I do remember falling out of a tree. Thank god     | I do remember falling out of a tree. Thank ___     | \n",
      "___ it wasnt ______ too high Reaching _____ ___    | God it _____ wasn't too ____ ________ high. You    | \n",
      "_____ ________ for the next one and the next one   | know, reaching for the next one and the next ___   | \n",
      "andall ____ ___ ____ ___ of a sudden ______        | ______ one, and then all of a sudden you're        | \n",
      "________ ___ you don't get it ___ and \"pew bam\"    | reaching and you don't get __ it, and ____ ____    | \n",
      "____ ____ It wasn't horrible _________ but it      | you, bam. It wasn't ________ horrible, but it      | \n",
      "hurt _____ ______ __ _____ _____ Did you break     | ____ hurt. Scary. It hurt, yeah. Did you break     | \n",
      "any thing? _________ No, I _______ ___ ____ never  | ___ ______ anything? No, I didn't. No, I've never  | \n",
      "had a broken bone _____ _____ Thank god  ____      | had a broken ____ bone. Good. Thank ___  God.      | \n",
      "_____                                              | Okay.                                              | \n"
     ]
    }
   ],
   "source": [
    "# lowest score\n",
    "evaltext(\"SS_IMG_2863\")\n",
    "comptext(\"SS_IMG_2863\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf979da-a5d3-4d51-bc68-3d4fd238a374",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84e8cd4-16e8-4dce-baca-94facd60031d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10020c94-724b-4d32-a2a1-430f8056a9f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3bdb8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7480e346",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7425edfa-85d1-48c0-a575-ed81389a86e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31495466-acbe-4ee7-b6ba-18a781f61247",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
